{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yelp Review Rating Prediction System\n",
    "Evaluates three different prompting approaches for LLM-based sentiment classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee47829",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a421c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "SAMPLE_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ab71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting Strategies\n",
    "PROMPT_APPROACH_1 = \"\"\"You are a restaurant review sentiment classifier. Your task is to read a review and predict the star rating (1-5).\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond ONLY with valid JSON in this exact format:\n",
    "{{\"predicted_stars\": <number 1-5>, \"explanation\": \"<brief reason>\"}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_APPROACH_2 = \"\"\"You are an expert review analyst. Analyze this review by examining these key aspects:\n",
    "1. Food Quality (taste, presentation, portions)\n",
    "2. Service Quality (speed, friendliness, attentiveness)\n",
    "3. Ambiance/Cleanliness (environment, hygiene, comfort)\n",
    "4. Value for Money (price vs quality)\n",
    "5. Overall Experience\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "For each aspect, identify positive/negative mentions. Then synthesize into an overall 1-5 star rating based on the balance of factors.\n",
    "\n",
    "Respond ONLY with valid JSON:\n",
    "{{\"predicted_stars\": <number 1-5>, \"explanation\": \"<brief reason>\"}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445ab5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "PROMPT_APPROACH_3 = \"\"\"You are an expert review classifier trained on thousands of restaurant reviews. Use these examples as reference:\n",
    "\n",
    "EXAMPLE 1 (5 stars):\n",
    "Review: \"Amazing food, great service, will come back!\"\n",
    "\n",
    "EXAMPLE 2 (3 stars):\n",
    "Review: \"Food was okay but service was slow.\"\n",
    "\n",
    "EXAMPLE 3 (1 star):\n",
    "Review: \"Worst experience ever. Rude staff, cold food.\"\n",
    "\n",
    "Now classify this review:\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond ONLY with valid JSON:\n",
    "{{\"predicted_stars\": <number 1-5>, \"explanation\": \"<brief reason>\"}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc361c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_yelp_dataset(csv_path: str, sample_size: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess Yelp reviews dataset.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[['text', 'stars']].dropna()\n",
    "    df = df.rename(columns={'text': 'review_text', 'stars': 'rating'})\n",
    "    df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725ad02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, max_retries: int = 3) -> Dict:\n",
    "    \"\"\"Execute LLM API call with retry logic and error handling.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"google/gemini-2.0-flash-exp:free\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            \n",
    "            try:\n",
    "                result = json.loads(response_text)\n",
    "                return {\"success\": True, \"data\": result, \"raw\": response_text}\n",
    "            except json.JSONDecodeError:\n",
    "                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                    return {\"success\": True, \"data\": result, \"raw\": response_text}\n",
    "                else:\n",
    "                    return {\"success\": False, \"error\": \"Invalid JSON\", \"raw\": response_text}\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return {\"success\": False, \"error\": str(e), \"raw\": None}\n",
    "    \n",
    "    return {\"success\": False, \"error\": \"Max retries exceeded\", \"raw\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ff7f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_approach(df: pd.DataFrame, approach_name: str, prompt_template: str) -> Dict:\n",
    "    \"\"\"Evaluate single prompting approach across dataset.\"\"\"\n",
    "    results = []\n",
    "    execution_times = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = prompt_template.format(review=row['review_text'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = call_llm(prompt)\n",
    "        execution_time = time.time() - start_time\n",
    "        execution_times.append(execution_time)\n",
    "        \n",
    "        if response[\"success\"]:\n",
    "            predicted = response[\"data\"].get(\"predicted_stars\")\n",
    "            explanation = response[\"data\"].get(\"explanation\", \"\")\n",
    "            is_valid_json = True\n",
    "        else:\n",
    "            predicted = None\n",
    "            explanation = response.get(\"error\", \"\")\n",
    "            is_valid_json = False\n",
    "        \n",
    "        results.append({\n",
    "            \"actual\": int(row['rating']),\n",
    "            \"predicted\": predicted,\n",
    "            \"explanation\": explanation,\n",
    "            \"valid_json\": is_valid_json,\n",
    "            \"execution_time\": execution_time\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"    Progress: {idx + 1}/{len(df)} reviews processed...\")\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    valid_results = [r for r in results if r[\"valid_json\"] and r[\"predicted\"] is not None]\n",
    "    correct = sum(1 for r in valid_results if r[\"actual\"] == r[\"predicted\"])\n",
    "    \n",
    "    accuracy = (correct / len(valid_results) * 100) if valid_results else 0\n",
    "    json_validity = (len(valid_results) / len(results) * 100)\n",
    "    avg_execution_time = sum(execution_times) / len(execution_times)\n",
    "    \n",
    "    differences = [abs(r[\"actual\"] - r[\"predicted\"]) for r in valid_results if r[\"predicted\"]]\n",
    "    consistency = sum(d == 0 for d in differences) / len(differences) * 100 if differences else 0\n",
    "    \n",
    "    return {\n",
    "        \"approach\": approach_name,\n",
    "        \"accuracy\": round(accuracy, 2),\n",
    "        \"json_validity\": round(json_validity, 2),\n",
    "        \"consistency\": round(consistency, 2),\n",
    "        \"avg_time_ms\": round(avg_execution_time * 1000, 2),\n",
    "        \"total_samples\": len(results),\n",
    "        \"valid_samples\": len(valid_results),\n",
    "        \"detailed_results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cbe09",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Execute evaluation workflow for all prompting approaches.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"YELP REVIEW RATING PREDICTION - PROMPTING APPROACHES EVALUATION\")\n",
    "    print(\"Using OpenRouter API\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n[1] Loading Yelp Reviews Dataset...\")\n",
    "    df = load_yelp_dataset(\"yelp_reviews_sample.csv\", sample_size=SAMPLE_SIZE)\n",
    "    print(f\"    Loaded {len(df)} reviews\")\n",
    "    print(f\"    Sample review: {df.iloc[0]['review_text'][:100]}...\")\n",
    "    \n",
    "    print(\"\\n[2] Evaluating Prompting Approaches...\")\n",
    "    \n",
    "    approaches = [\n",
    "        (\"Approach 1: Direct Prompting\", PROMPT_APPROACH_1),\n",
    "        (\"Approach 2: Chain-of-Thought\", PROMPT_APPROACH_2),\n",
    "        (\"Approach 3: Few-Shot Prompting\", PROMPT_APPROACH_3),\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for approach_name, prompt_template in approaches:\n",
    "        print(f\"\\n    Evaluating {approach_name}...\")\n",
    "        result = evaluate_approach(df, approach_name, prompt_template)\n",
    "        evaluation_results.append(result)\n",
    "        print(f\"    Accuracy: {result['accuracy']}%\")\n",
    "        print(f\"    JSON Validity: {result['json_validity']}%\")\n",
    "        print(f\"    Consistency: {result['consistency']}%\")\n",
    "    \n",
    "    print(\"\\n[3] Comparison Table\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Approach':<30} {'Accuracy':<12} {'JSON Valid':<12} {'Consistency':<12} {'Avg Time'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        print(f\"{result['approach']:<30} {result['accuracy']:<12}% {result['json_validity']:<12}% {result['consistency']:<12}% {result['avg_time_ms']}ms\")\n",
    "    \n",
    "    print(\"\\n[4] Saving Results...\")\n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(evaluation_results, f, indent=2, default=str)\n",
    "    print(f\"    Results saved to evaluation_results.json\")\n",
    "    \n",
    "    best = max(evaluation_results, key=lambda x: x[\"accuracy\"])\n",
    "    print(f\"\\n[5] Best Approach: {best['approach']} (Accuracy: {best['accuracy']}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Evaluation complete! Check evaluation_results.json for details.\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
